# Technical Documentation Workflow
# Generate documentation about a codebase topic with multi-source research

[workflow]
name = "tech-docs"
description = "Create technical documentation with multi-source research and review"

[[phases]]
id = "research-planning"
prompt_files = ["research-planning"]
context_artifacts = ["research"]
required_artifacts = ["research-plan"]
prompt = """
## Tasks

1. Identify what needs to be researched for this documentation topic
2. List specific questions to answer
3. Identify sources (codebase, web, docs, GitHub issues/PRs)

## Source Hierarchy (Code is Truth)

| Priority | Source | Role | Validation |
|----------|--------|------|------------|
| 1 | Current source code | Ground truth | None needed |
| 2 | Official documentation | Design intent | Verify against code |
| 3 | GitHub issues/PRs (recent, closed) | Historical context | Verify current state |
| 4 | GitHub issues/PRs (old, open) | Low confidence | Likely stale |
| 5 | External resources | Background | Always verify |

**Core principle:** If documentation contradicts code, code wins. Claims from sources 2-5 must be verified.

## Outputs

Write `research-plan.md` with:
- Questions to answer (grouped by domain)
- Sources to consult (prioritized per hierarchy above)
- Research task assignments
"""
suggested_next = [
    { phase = "research", instruction = "Research plan ready — begin research" }
]

[[phases]]
id = "research"
use_tasks = true
supports_prototypes = true
supports_cache_reference = true
context_artifacts = ["research-plan", "research"]
required_artifacts = ["research"]
prompt = """
# Research Phase

Execute the research plan (injected above).

If prior research findings are injected above (from a previous iteration), build on them — add new findings without removing existing content.

## Target Audience

Write for developers who:
- Know the programming language and framework basics
- Have not used this specific codebase before
- Will search for specific answers, not read front-to-back

Do NOT write for beginners, experts, or managers.

## Tasks

1. Explore the codebase for relevant code
2. Search for existing documentation
3. Check comments and docstrings
4. Look for examples and tests
5. Optionally search web for related concepts
6. Check GitHub PRs/issues for context (use `gh` CLI)

## Version Anchoring

Record the current git commit for version anchoring:
```bash
git rev-parse HEAD
```

This commit hash will appear in the generated documentation.

## Outputs

Write `research.md` with:
- Code locations and patterns found
- Existing documentation snippets
- Example usages
- External references
- Questions that need answers
- Git commit hash for version anchor

If this is a subsequent iteration, append new findings to the existing research.md content.
"""
suggested_next = [
    { phase = "evaluate-research", instruction = "Research complete — evaluate findings" }
]

[[phases]]
id = "evaluate-research"
max_retries = 4
prompt_files = ["evaluate-research"]
context_artifacts = ["research-plan", "research"]
suggested_next = [
    { phase = "draft", instruction = "All questions adequately answered" },
    { phase = "research-planning", instruction = "Significant gaps remain — plan additional research for unanswered questions" }
]

[[phases]]
id = "draft"
prompt = """
# Draft Phase

Create the initial documentation following strict guidelines.

## Inputs

Review research.md for gathered information.

## Document Structure

1. **Purpose** - One sentence
2. **Quick Start** - Minimal working example
3. **Examples** - 2-3 common patterns
4. **Details** - Deeper explanation
5. **See Also** - Cross-references

Skip empty sections.

## Example-Based Documentation Pattern

**Bad (exhaustive list):**
```markdown
## Hooks
Available hooks: PreToolUse, PostToolUse, Stop, PreCompact, SessionStart, Notification, SubagentStop, SubagentStart
```

**Good (example + search):**
```markdown
## Hooks

The SessionStart hook runs at conversation start:

```python
def session_start_hook():
    print("Session started")
```

To find all hooks, search for `hook_type` in hooks.py.
```

## Cross-Reference Guidelines

To source code:
```markdown
See [`src/parser.py:45-60`](../src/parser.py) for tokenization.
```

To related documentation:
```markdown
For configuration, see [config.md](./config.md).
```

To external resources:
```markdown
For regex syntax, see [Python re module](https://docs.python.org/3/library/re.html).
```

## Durable Documentation

Write documentation that survives code changes:

**Fragile:**
```markdown
The third argument controls buffering.
```

**Durable:**
```markdown
Pass `buffer_size` to control memory. Search for "buffer_size" in file_processor.py.
```

Principles:
- Reference by name, not position
- Include search terms for discovery
- Point to source for exhaustive details

## Negative Space Documentation

Document what the system does NOT do only when:
- A developer would reasonably assume it does
- The misconception leads to bugs or wasted debugging
- It's a deliberate design choice, not missing feature

**Good:** Note: The cache does not persist across restarts.
**Bad:** Note: This function does not make coffee.

## Version Anchoring

Include at top of generated documentation:
```markdown
---
generated_from_commit: [full SHA]
generated_date: [YYYY-MM-DD]
---
```

## Outputs

Write draft documentation file (e.g., `topic.md`):
- Overview/introduction
- Core concepts
- Usage examples
- API reference (if applicable)
- Troubleshooting/FAQ
"""
suggested_next = [
    { phase = "review", instruction = "Draft ready — send for external review" }
]

[[phases]]
id = "review"
prompt_files = ["selective-re-review"]
context_artifacts = ["review-feedback"]
required_artifacts = ["review-feedback"]
prompt = """
# Review Phase

Get comprehensive feedback on documentation quality.

Link validation and slop detection are cheap — always re-run these on re-review.
Only skip gemini-reviewer or codex-reviewer if their Critical items are all resolved.

## Tasks

Run these reviews in parallel (on re-review, skip resolved reviewers):

### 1. Link Validation
Verify all source file links resolve:
- Extract all markdown links from draft
- Verify source file links exist
- Check line number references are valid
- Report broken links with suggested fixes

### 2. Slop Detection
Scan for AI-generated writing patterns using the comprehensive slop detection guide.
Create a task with `"prompt_file": "slop-detection"` to inject the full detection patterns.

Key patterns for documentation:
- AI vocabulary words (delve, utilize, leverage, ensure, robust, etc.)
- Throat-clearing phrases ("In order to", "It's worth noting")
- Artificial transitions ("Furthermore", "Moreover")
- Em-dash abuse, passive voice, summary addiction
- Hazy claims of importance

**Voice requirements:**
- Active: "The parser reads" not "is read by"
- Short sentences
- Plain words: "use" not "utilize"

### 3. Gemini Editorial Review
Launch gemini-reviewer for content review:
- Evaluate document structure and navigation
- Check logical flow between sections
- Identify gaps (unanswered reader questions)
- Verify examples appear before explanations
- Tag each item as **Critical**, **Suggestion**, or **Nit**

### 4. Codex Accuracy Review
Launch codex-reviewer for technical accuracy:
- Verify code examples compile/parse
- Check function signatures match code
- Verify claimed behavior matches implementation
- Check configuration options exist
- Flag unverifiable claims
- Tag each item as **Critical**, **Suggestion**, or **Nit**

## Outputs

Write `review-feedback.md` with severity-tagged feedback organized by source:
- Link validation results (Critical for broken links)
- Slop detection findings (Critical for high-severity slop)
- Content feedback from gemini-reviewer (Critical/Suggestion/Nit)
- Technical accuracy issues from codex-reviewer (Critical/Suggestion/Nit)
"""
suggested_next = [
    { phase = "revise", instruction = "Feedback received — incorporate changes" }
]

[[phases]]
id = "revise"
context_artifacts = ["review-feedback"]
prompt_files = ["revise"]
prompt = """
## Inputs

- Draft documentation from draft phase
- review-feedback.md (injected above)

Priority order: correctness > broken links > completeness > clarity > style

## Tasks

1. Apply all ACCEPT feedback items
2. Investigate uncertain items, update docs based on findings
3. Fix any broken links
4. Remove slop patterns
5. Verify version anchor is present

## Outputs

Updated documentation with addressed feedback.
"""
suggested_next = [
    { phase = "evaluate-revisions", instruction = "Revisions complete — evaluate changes" }
]

[[phases]]
id = "evaluate-revisions"
max_retries = 2
prompt_files = ["evaluate-revisions"]
context_artifacts = ["review-feedback"]
suggested_next = [
    { phase = "complete", instruction = "All critical items resolved" },
    { phase = "review", instruction = "Critical items remain unresolved — re-engage only reviewers/checks with unresolved Critical items" }
]

[[phases]]
id = "complete"
prompt = """
# Complete Phase

Finalize the documentation.

## Tasks

1. Final review of documentation
2. Verify formatting is consistent
3. Note where to publish/commit
4. Document any follow-up work

## Documentation Checklist

- [ ] Version anchor present (commit hash + date)
- [ ] All links verified
- [ ] No slop patterns
- [ ] Examples before explanations
- [ ] Cross-references to source code
- [ ] Target audience appropriate

This is a terminal phase. The documentation is complete.
"""
terminal = true
