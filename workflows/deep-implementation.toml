# Deep Implementation Workflow
# Complex implementation with thorough research, external review, and feedback synthesis

[workflow]
name = "deep-implementation"
description = "Complex features requiring thorough exploration and multi-round feedback"

[[phases]]
id = "research-planning"
prompt_files = ["research-planning"]
context_artifacts = ["research-findings"]
required_artifacts = ["research-plan"]
prompt = """
## Tasks

1. Identify what needs to be researched for this specific topic
2. List specific questions to answer
3. Identify potential sources (codebase, web, docs)
4. Create research task list

## Exploration Angles to Consider

**Determine what needs exploration** for this specific topic. Common angles include:
- **Codebase patterns** — Existing implementations, architectural conventions, file organization
- **External approaches** — Web research on best practices, libraries, how others solve this
- **Requirements analysis** — Constraints, edge cases, acceptance criteria
- **Prototyping** — Quick spikes to validate assumptions (write to plan directory only)
- **API/library research** — Documentation, capabilities, integration patterns
- **Performance considerations** — Bottlenecks, scaling concerns, benchmarks

## Example Research Patterns

**Simple feature** (2-3 areas):
- Codebase patterns + requirements analysis

**Integration feature** (3-4 areas):
- Codebase patterns + external API docs + requirements + compatibility research

**Architectural change** (4-6 areas):
- Current architecture analysis + alternative approaches research + performance implications + migration patterns + requirements

## Outputs

Write `research-plan.md` with:
- Questions to answer (grouped by domain)
- Sources to consult for each
- Expected findings
- Research task assignments
"""
suggested_next = [
    { phase = "research", instruction = "Research plan ready — begin research" }
]

[[phases]]
id = "research"
use_tasks = true
supports_prototypes = true
supports_cache_reference = true
context_artifacts = ["research-plan", "research-findings"]
required_artifacts = ["research-findings"]
required_json_artifacts = ["cache-candidates"]
prompt = """
# Research Phase

Execute the research plan (injected above) with multiple parallel investigations.

If prior research findings are injected above (from a previous iteration), build on them — add new findings without removing existing content.

## Tasks

Launch exploration agents. **You decide** how many agents to launch based on the topic's complexity.

1. Launch agents for codebase investigation
2. Use WebSearch/WebFetch for external research
3. Check documentation and examples
4. Synthesize findings from all sources

**Launch agents appropriately:**
- Run independent explorations in parallel for speed
- Chain explorations that depend on each other (e.g., "research library X" → "prototype with library X")
- Use more agents for complex topics with many angles
- Use fewer for focused topics with clear scope

## Model Selection

| Task Type | Model |
|-----------|-------|
| Codebase research | `opus` |
| External API/library/web research | `haiku` |
| Prototyping spike | `sonnet` |

## Outputs

Write `research-findings.md` with:
- Answers to each question from research-plan
- Code patterns found
- External references
- Remaining unknowns

Use prototype tasks if you need to answer questions through implementation.

If this is a subsequent iteration, append new findings to the existing research-findings.md content.
"""
suggested_next = [
    { phase = "evaluate-research", instruction = "Research complete — evaluate findings" }
]

[[phases]]
id = "evaluate-research"
max_retries = 4
prompt_files = ["evaluate-research"]
context_artifacts = ["research-plan", "research-findings"]
suggested_next = [
    { phase = "synthesize", instruction = "All questions adequately answered" },
    { phase = "research-planning", instruction = "Significant gaps remain — plan additional research focusing only on unanswered questions" }
]

[[phases]]
id = "synthesize"
context_artifacts = ["research-findings"]
required_artifacts = ["implementation-plan", "verification-plan"]
prompt = """
# Synthesize Phase

Create implementation plan from the research findings (injected above).

## Tasks

1. Identify key insights from each exploration
2. Determine implementation approach
3. Draft task breakdown with dependencies
4. Assess confidence (1-5 scale)

## Confidence Assessment Format

Include this section in `implementation-plan.md`:

```markdown
## Confidence Assessment

**Overall Score: [1-5]**

| Dimension | Score | Explanation |
|-----------|-------|-------------|
| Feasibility | [1-5] | Can this be implemented with the current codebase? |
| Scope | [1-5] | Is the scope well-defined and achievable? |
| Technical Risk | [1-5] | Are there unknowns or risky assumptions? |

**Areas of Uncertainty:**
- [List specific uncertainties to resolve via review]

**Questions for Reviewers:**
- [Specific questions you want external reviewers to address]
```

## Parallelization Rules

Tasks without parent dependencies can run in parallel, but **only if they won't mutate files in the same directories**.

**Safe to parallelize** (no parent dependency needed):
- Research tasks that only write to their own `tasks/[task-id]/` output directory
- Monorepo tasks that modify separate packages
- Tasks that only read from the codebase without writing

**Must be sequential** (add parent dependency):
- Multiple tasks editing `src/` files
- Tasks that both modify config files
- Implementation tasks that build on each other's code

## Checkpoint

**If overall confidence score is below 4**, STOP and discuss concerns with the user before proceeding:
- Present the confidence assessment
- List specific concerns and dimensions
- Ask for guidance: proceed anyway, investigate further, or pivot

## Outputs

### implementation-plan.md
- Recommended approach and rationale
- Task breakdown with dependencies (as markdown, not tasks.json yet)
- Confidence assessment

### verification-plan.md
Create a verification plan based on what's available:

```markdown
# Verification Plan

## Available Verification Methods
Analyze and document:
- Test suite? (`npm test`, `pytest`, `go test`, etc.)
- Type checking? (`tsc`, `mypy`, `pyright`, etc.)
- Linting? (`eslint`, `ruff`, etc.)
- Build? (`npm run build`, `make`, etc.)
- MCP tools? (browser automation, API testing, etc.)

## Automated Checks
- [ ] `<command>` - <what it verifies>

## Interactive Verification (if MCP available)
- [ ] Navigate to <url>, verify <expected behavior>

## Acceptance Criteria
- <criterion from request>
```
"""
suggested_next = [
    { phase = "review", instruction = "Plan ready — send for external review" }
]

[[phases]]
id = "review"
prompt_files = ["selective-re-review"]
context_artifacts = ["implementation-plan", "verification-plan", "review-feedback"]
required_artifacts = ["review-feedback"]
prompt = """
# Review Phase

Get multiple external perspectives on the plan.

## Tasks

1. Launch gemini-reviewer for plan review (skip on re-review if no unresolved Critical items from gemini)
2. Launch codex-reviewer for second opinion (skip on re-review if no unresolved Critical items from codex)
3. Collect all feedback with severity levels

**Important for reviewer tasks:**
- Do NOT include `model` field (reviewers use their own external models)
- Do NOT include `subagent_prompt` field (not used for reviewers)

**Reviewer prompts should focus on:**

**gemini-reviewer:**
- Is the plan complete? Any missing steps?
- Are there blind spots or unconsidered edge cases?
- Is the task breakdown logical and well-sequenced?
- Are dependencies correctly identified?
- Tag each item as **Critical**, **Suggestion**, or **Nit**

**codex-reviewer:**
- Is the technical approach sound?
- Are there better architectural choices?
- Any potential implementation pitfalls?
- Are the tasks appropriately scoped?
- Tag each item as **Critical**, **Suggestion**, or **Nit**

## Outputs

Write `review-feedback.md` with severity-tagged feedback organized by source (Critical/Suggestion/Nit).
"""
suggested_next = [
    { phase = "revise", instruction = "Feedback received — incorporate changes" }
]

[[phases]]
id = "revise"
context_artifacts = ["implementation-plan", "review-feedback"]
required_artifacts = ["implementation-plan"]
prompt_files = ["revise"]
prompt = """
## Inputs

- implementation-plan.md (injected above)
- review-feedback.md (injected above)

## Tasks

1. Update implementation-plan.md with accepted changes
2. Re-assess confidence

## Investigation Findings Format

If investigations were needed, write `investigation-findings.md`:

```markdown
# Investigation Findings

## Question: [Original question]
**Domain:** codebase|technical|requirements
**Finding:** [What was discovered]
**Impact on Plan:** [How this affects the implementation plan]
**Recommendation:** accept-feedback|reject-feedback|modify-approach
```

## Outputs

Updated implementation-plan.md with addressed feedback and updated confidence score.
"""
suggested_next = [
    { phase = "evaluate-revisions", instruction = "Revisions complete — evaluate changes" }
]
on_blocked = "synthesize"

[[phases]]
id = "evaluate-revisions"
max_retries = 2
prompt_files = ["evaluate-revisions"]
context_artifacts = ["implementation-plan", "review-feedback"]
prompt = """
Also check the confidence score — if below 4, note this as a concern.
"""
suggested_next = [
    { phase = "implement", instruction = "All critical items resolved and confidence >= 4" },
    { phase = "review", instruction = "Critical items remain unresolved — re-engage only reviewers with unresolved Critical items" },
    { phase = "synthesize", instruction = "Confidence < 4 despite revisions — rethink approach" },
    { phase = "research", instruction = "Fundamental gaps require additional investigation", requires_approval = true, approval_prompt = "Return to research for additional investigation?" }
]

[[phases]]
id = "implement"
use_tasks = true
on_blocked = "self"
max_retries = 3
context_artifacts = ["implementation-plan"]
required_json_artifacts = ["proposals", "challenges"]
prompt_files = ["implement"]
prompt = """
## Task Schema - context_artifacts

When creating tasks, use `context_artifacts` to give tasks access to relevant documents:

```json
{
  "id": "implement-feature",
  "description": "Implement the feature",
  "context_artifacts": ["request", "implementation-plan"],
  ...
}
```

**When to use context_artifacts:**
- Implementation tasks → `["request", "implementation-plan"]`
- Complex tasks → `["request", "implementation-plan", "categorized-feedback"]`
- Validation tasks → `["request"]` (to verify against requirements)

Common artifacts in this workflow:
- `request` - Original user request
- `implementation-plan` - Synthesized plan with task breakdown
- `categorized-feedback` - External reviewer feedback

Use dead-end tracking for failed approaches.

## Model Selection for Tasks

| Task Type | Model |
|-----------|-------|
| Simple file changes | `sonnet` (default) |
| Complex implementation | `opus` |
| Architecture decisions | `opus` |
| Boilerplate/scaffolding | `haiku` |
| Test writing | `sonnet` |
| Documentation | `haiku` |
"""
suggested_next = [
    { phase = "validate", instruction = "All tasks complete — run verification" },
    { phase = "revise", instruction = "Reassess plan based on implementation findings", requires_approval = true, approval_prompt = "Return to revise to reassess based on implementation findings?" }
]

[[phases]]
id = "validate"
prompt_files = ["validate"]
context_artifacts = ["verification-plan"]
prompt = """
Also check edge cases identified during research phase.
If a fundamental issue is found, record it in dead-ends.
"""
suggested_next = [
    { phase = "complete", instruction = "All checks pass" },
    { phase = "revise", instruction = "Reassess plan based on validation findings", requires_approval = true, approval_prompt = "Return to revise to reassess based on validation findings?" }
]
on_blocked = "implement"

[[phases]]
id = "complete"
prompt_files = ["proposals-and-challenges", "complete-implementation"]
prompt = """
## Clean Up

Remove intermediate artifacts if no longer needed:
- Draft plans
- Categorized feedback
- Investigation findings

(Or keep them for audit trail)
"""
terminal = true
