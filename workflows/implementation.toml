# Implementation Workflow
# A straightforward workflow for implementing features with research and planning

[workflow]
name = "implementation"
description = "Build features, fix bugs, implement changes with research and validation"

[[phases]]
id = "research"
use_tasks = true
supports_prototypes = true
supports_cache_reference = true
max_iterations = 4
prompt = """
# Research Phase

Your goal is to understand the codebase and requirements before implementation.

## Tasks
Create research tasks to:
1. Explore the codebase to understand existing patterns and architecture
2. Identify files and components relevant to this request
3. Look for similar implementations to follow as examples
4. Note any constraints, dependencies, or potential blockers

Use prototype tasks if you need to answer questions through implementation (e.g., "Can library X work with Y?").

## Outputs
Synthesize findings into `research.md` with one section per task. Include:
- Brief overview of important findings
- Links to full reports in task directories
"""
suggested_next = ["research", "plan"]

[[phases]]
id = "plan"
use_tasks = true
prompt = """
# Plan Phase

Create a detailed implementation plan based on exploration findings.

## Inputs
Review the research.md from the research phase.

## Planning Tasks
Create tasks in this phase to:
1. Analyze findings and identify implementation approach
2. Break down the work into atomic, testable tasks
3. Identify which tasks can be parallelized
4. Consider test-first tasks if acceptance criteria are clear

## Outputs

### implementation-plan.md
- Recommended approach and rationale
- Task breakdown (descriptions, dependencies, steps)
- Parallelization strategy
- Risk assessment

### verification-plan.md
Create a verification plan based on what's available in the project:

```markdown
# Verification Plan

## Available Verification Methods
Analyze and document what verification is possible:
- Test suite? (`npm test`, `pytest`, `go test`, etc.)
- Type checking? (`tsc`, `mypy`, `pyright`, etc.)
- Linting? (`eslint`, `ruff`, etc.)
- Build verification? (`npm run build`, `make`, etc.)
- MCP tools? (browser automation, API testing, etc.)

## Automated Checks
List specific commands to run:
- [ ] `<command>` - <what it verifies>

## Interactive Verification (if MCP available)
If browser/UI MCP servers are available, list verification steps:
- [ ] Navigate to <url>, verify <expected behavior>

## Acceptance Criteria
What must be true for implementation to be complete?
```

Record both artifacts:
```bash
uv run plan.py record-artifact implementation-plan <path>
uv run plan.py record-artifact verification-plan <path>
```

Note: The implement phase will create its own tasks.json from the implementation plan.
"""
suggested_next = ["implement"]

[[phases]]
id = "implement"
use_tasks = true
prompt = """
# Implement Phase

Execute the implementation plan.

## Inputs
- Implementation plan from the plan phase
- findings.md from explore phase

## Setup
1. Read the implementation plan from the plan phase
2. Create tasks.json in this phase directory based on the plan
3. All tasks start with `status: "todo"`

## Task Schema - context_artifacts

When creating tasks, use `context_artifacts` to give tasks access to relevant documents:

```json
{
  "id": "implement-auth",
  "description": "Implement authentication",
  "context_artifacts": ["request", "findings"],
  ...
}
```

**When to use context_artifacts:**
- Research/exploration tasks → `["request"]` (need the original requirements)
- Implementation tasks → `["request", "findings"]` or `["request", "design"]`
- Validation tasks → `["request"]` (to verify against requirements)

Artifacts must be recorded via `record-artifact` to be available. Common artifacts:
- `request` - Original user request
- `findings` - Exploration/research findings
- `design` - Design document (if design phase exists)

## Task Execution
1. Work through tasks in dependency order
2. Set each task to in-progress before starting
3. Mark tasks done immediately after completion
4. Log progress for each significant step

## Guidelines
- Follow existing code patterns identified in explore
- Make minimal, focused changes
- Don't over-engineer or add unnecessary features
- Test changes as you go
"""
suggested_next = ["validate"]

[[phases]]
id = "validate"
prompt = """
# Validate Phase

Verify the implementation meets requirements using the verification plan.

## Inputs
Retrieve the verification plan:
```bash
uv run plan.py get-user-guidance  # Check for any user-provided focus areas
```

The verification-plan artifact contains the checks to run.

## Execution

### 1. Run Automated Checks
Execute each command from the verification plan's "Automated Checks" section:
- Run test suite
- Run type checker
- Run linter
- Run build

Log results for each check.

### 2. Interactive Verification (if applicable)
If the verification plan includes MCP-based checks:
- Execute browser automation steps
- Verify UI behavior
- Test API endpoints

### 3. Acceptance Criteria
Verify each criterion from the verification plan is met.

## Outcomes

**All checks pass:** Transition to complete.

**Checks fail:**
- If fixable: Fix and re-run checks
- If needs significant work: Transition back to implement with notes on what failed
"""
suggested_next = ["complete", "implement"]
on_blocked = "implement"

[[phases]]
id = "complete"
prompt = """
# Complete Phase

Finalize the implementation.

## Tasks
1. Review all changes made
2. Ensure tests pass
3. Prepare commit message summarizing changes
4. Note any follow-up work needed

This is a terminal phase. The workflow is complete.
"""
terminal = true
