# Iteration Workflow
# Iterative execution with evaluate-generate loops for long-horizon goals

[workflow]
name = "iteration"
description = "Iterative execution with evaluate-generate loops for long-horizon goals"

[[phases]]
id = "research-planning"
prompt_files = ["research-planning"]
context_artifacts = ["research-brief", "research"]
required_artifacts = ["research-plan"]
prompt = """
## Tasks

1. Identify what needs to be researched for this goal
2. List specific questions to answer
3. Include: acceptance criteria, verification methods, scope assessment

## Outputs

Write `research-plan.md` with:
- Questions to answer (grouped by domain)
- Sources to consult
- Research task assignments
"""
suggested_next = [
    { phase = "research", instruction = "Research plan ready — begin research" }
]

[[phases]]
id = "research"
use_tasks = true
supports_prototypes = true
supports_cache_reference = true
context_artifacts = ["research-plan", "research"]
required_artifacts = ["research"]
prompt = """
# Research Phase

Execute the research plan (injected above).

If prior research findings are injected above (from a previous iteration), build on them — add new findings without removing existing content.

## Tasks

Create research tasks to:
1. Understand the problem domain and current state
2. Identify acceptance criteria for the goal
3. Discover available verification methods (tests, linting, type checks, manual verification)
4. Map out the scope and potential approaches

## Model Selection

| Task Type | Model |
|-----------|-------|
| Codebase research | `opus` |
| External API/library/web research | `haiku` |
| Prototyping spike | `sonnet` |

## Outputs

Write `research.md` with:
- Problem domain understanding
- Acceptance criteria for the goal
- Available verification methods and commands
- Scope assessment and potential approaches

If this is a subsequent iteration, append new findings to the existing research.md content.
"""
suggested_next = [
    { phase = "evaluate-research", instruction = "Research complete — evaluate findings" }
]

[[phases]]
id = "evaluate-research"
max_retries = 4
prompt_files = ["evaluate-research"]
context_artifacts = ["research-plan", "research"]
suggested_next = [
    { phase = "plan", instruction = "All questions adequately answered" },
    { phase = "research-planning", instruction = "Significant gaps remain — plan additional research for unanswered questions" }
]

[[phases]]
id = "plan"
requires_user_input = true
context_artifacts = ["research"]
required_artifacts = ["implementation-plan"]
prompt = """
# Plan Phase

Create the initial execution plan based on the research findings (injected above).

## Define

1. The goal and acceptance criteria (from research)
2. The first batch of tasks to execute
3. How progress will be evaluated after each batch

## Outputs

### implementation-plan.md

Write the execution strategy:
- Goal statement and acceptance criteria
- First batch of tasks with descriptions and steps
- Evaluation approach: what commands to run, what to check
- Expected number of iterations (estimate)

Note: The execute phase will create its own tasks.json from this plan.
"""
suggested_next = [
    { phase = "execute", instruction = "Plan approved — begin execution" },
    { phase = "plan", instruction = "Revise the plan" },
    { phase = "research", instruction = "Return to research for additional investigation", requires_approval = true, approval_prompt = "Return to research for additional investigation?" }
]

[[phases]]
id = "execute"
use_tasks = true
max_retries = 20
context_artifacts = ["research", "implementation-plan"]
required_json_artifacts = ["proposals", "challenges"]
required_artifacts = ["execution-results"]
prompt = """
# Execute Phase

Execute the current batch of tasks from tasks.json.

## Setup

If tasks.json is empty, create tasks based on the implementation plan (injected above).
For re-entries, tasks.json may already contain tasks generated by the generate-tasks phase.

## Task Execution

1. Work through tasks in dependency order
2. Set each task to in-progress before starting
3. Mark tasks done immediately after completion
4. Log progress for each significant step

## After All Tasks Complete

Write `execution-results.md` summarizing:
- What each task accomplished
- Key findings and observations
- Any issues encountered

Then transition to evaluate.

## Important Notes

- If a task blocks, record what failed and transition to evaluate rather than
  retrying blindly. The evaluate phase will generate corrected tasks.
- Each re-entry to this phase creates a new phase directory. Previous execution
  results are preserved for audit trail.
"""
suggested_next = [
    { phase = "evaluate", instruction = "Execution complete — evaluate progress" },
    { phase = "execute", instruction = "Task blocked — retry with adjusted approach" }
]

[[phases]]
id = "evaluate"
context_artifacts = ["research", "implementation-plan", "execution-results", "evaluation"]
required_artifacts = ["evaluation"]
prompt = """
# Evaluate Phase

Evaluate progress against the acceptance criteria from the plan.

## IMPORTANT: Run Verification First

Run verification commands FIRST. Report their actual output.
Do NOT claim tests pass based on reasoning alone. Always execute:
- Tests (if applicable): run the actual test suite
- Type checking (if applicable): run the type checker
- Linting (if applicable): run the linter
- Manual inspection of outputs

## Then Assess Progress

Based on ACTUAL tool output, determine:
1. Which acceptance criteria are met?
2. Which are not yet met?
3. What new work is needed based on what was learned?

## Outputs

Write a CUMULATIVE `evaluation.md` that includes:
- Current verification results (tool output, not reasoning)
- Summary of ALL previous iterations (what was tried, what worked, what didn't)
- Unmet criteria with specific analysis of why they remain

## Decision

- If ALL acceptance criteria are met: transition to **complete**
- If criteria remain unmet: transition to **generate-tasks**
"""
suggested_next = [
    { phase = "complete", instruction = "Goal achieved — all acceptance criteria met" },
    { phase = "generate-tasks", instruction = "Progress made but goal not yet met — plan next iteration" }
]

[[phases]]
id = "generate-tasks"
context_artifacts = ["research", "implementation-plan", "evaluation"]
use_tasks = false
prompt = """
# Generate Tasks Phase

Based on the evaluation (injected above), generate the next batch of tasks.

## Read the Evaluation

Understand from the evaluation artifact:
- What acceptance criteria are not yet met
- What was learned during ALL previous iterations (evaluation includes cumulative history)
- What new work is needed
- What approaches have already been tried (do NOT repeat them)

## Check Dead-Ends

Dead-ends from failed approaches are injected above (if any exist).
Do NOT generate tasks that repeat failed approaches.

## Generate Tasks

Create tasks.json with tasks targeting the unmet criteria.
Each task should be scoped to fit in a single agent session (~10 steps max).

## Transition

After writing tasks.json, transition back to execute:

```bash
uv run plan.py loop-to-phase execute --reason "Iteration N: <summary of unmet criteria>"
```

This ensures proper re-entry context is maintained.
"""
suggested_next = [
    { phase = "execute", instruction = "Tasks generated — begin next iteration" }
]

[[phases]]
id = "complete"
terminal = true
prompt_files = ["proposals-and-challenges"]
context_artifacts = ["evaluation"]
prompt = """
# Complete Phase

All acceptance criteria met. Summarize the work completed.

## Final Summary

Summarize:
- The goal and how it was achieved
- Number of iterations taken
- Key learnings from the iteration journey
- Any follow-up work recommended

This is a terminal phase. The workflow is complete.
"""
