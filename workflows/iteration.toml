# Iteration Workflow
# Iterative execution with evaluate-generate loops for long-horizon goals

[workflow]
name = "iteration"
description = "Iterative execution with evaluate-generate loops for long-horizon goals"

[[phases]]
id = "research-planning"
context_artifacts = ["research"]
required_artifacts = ["research-plan"]
prompt = """
# Research Planning Phase

Plan the research before diving in.

## Prior Research

If prior research findings are injected above (from a previous iteration), review them to identify what's already been answered. Focus the new research plan on unanswered questions only.

## Tasks

1. Identify what needs to be researched for this goal
2. List specific questions to answer
3. Include: acceptance criteria, verification methods, scope assessment

## Outputs

Write `research-plan.md` with:
- Questions to answer (grouped by domain)
- Sources to consult
- Research task assignments
"""
suggested_next = ["research"]

[[phases]]
id = "research"
use_tasks = true
supports_prototypes = true
supports_cache_reference = true
context_artifacts = ["research-plan", "research"]
required_artifacts = ["research"]
prompt = """
# Research Phase

Execute the research plan (injected above).

If prior research findings are injected above (from a previous iteration), build on them â€” add new findings without removing existing content.

## Tasks

Create research tasks to:
1. Understand the problem domain and current state
2. Identify acceptance criteria for the goal
3. Discover available verification methods (tests, linting, type checks, manual verification)
4. Map out the scope and potential approaches

## Model Selection

| Task Type | Model |
|-----------|-------|
| Codebase research | `opus` |
| External API/library/web research | `haiku` |
| Prototyping spike | `sonnet` |

## Outputs

Write `research.md` with:
- Problem domain understanding
- Acceptance criteria for the goal
- Available verification methods and commands
- Scope assessment and potential approaches

If this is a subsequent iteration, append new findings to the existing research.md content.
"""
suggested_next = ["evaluate-research"]

[[phases]]
id = "evaluate-research"
max_retries = 4
context_artifacts = ["research-plan", "research"]
prompt = """
# Evaluate Research Phase

Evaluate whether the research adequately answers the questions from the research plan.

## Process

1. Read the research plan (injected above) and list each question
2. Read the research findings (injected above)
3. For each question, assess: answered, partially answered, or unanswered

## Decision

- If all questions are adequately answered: transition to **plan**
- If significant gaps remain: transition to **research-planning** to plan additional research
"""
suggested_next = ["plan", "research-planning"]

[[phases]]
id = "plan"
requires_user_input = true
context_artifacts = ["research"]
required_artifacts = ["implementation-plan"]
prompt = """
# Plan Phase

Create the initial execution plan based on the research findings (injected above).

## Define

1. The goal and acceptance criteria (from research)
2. The first batch of tasks to execute
3. How progress will be evaluated after each batch

## Outputs

### implementation-plan.md

Write the execution strategy:
- Goal statement and acceptance criteria
- First batch of tasks with descriptions and steps
- Evaluation approach: what commands to run, what to check
- Expected number of iterations (estimate)

Note: The execute phase will create its own tasks.json from this plan.
"""
suggested_next = [
    "execute",
    "plan",
    { phase = "research", requires_approval = true, approval_prompt = "Return to research for additional investigation?" }
]

[[phases]]
id = "execute"
use_tasks = true
on_blocked = "self"
max_retries = 20
context_artifacts = ["research", "implementation-plan"]
required_json_artifacts = ["proposals", "challenges"]
required_artifacts = ["execution-results"]
prompt = """
# Execute Phase

Execute the current batch of tasks from tasks.json.

## Setup

If tasks.json is empty, create tasks based on the implementation plan (injected above).
For re-entries, tasks.json may already contain tasks generated by the generate-tasks phase.

## Task Execution

1. Work through tasks in dependency order
2. Set each task to in-progress before starting
3. Mark tasks done immediately after completion
4. Log progress for each significant step

## After All Tasks Complete

Write `execution-results.md` summarizing:
- What each task accomplished
- Key findings and observations
- Any issues encountered

Then transition to evaluate.

## Important Notes

- `on_blocked` is advisory. If a task blocks, record what failed and transition
  to evaluate rather than waiting for automatic retry. The evaluate phase will
  generate corrected tasks.
- Each re-entry to this phase creates a new phase directory. Previous execution
  results are preserved for audit trail.
"""
suggested_next = ["evaluate"]

[[phases]]
id = "evaluate"
context_artifacts = ["research", "implementation-plan", "execution-results", "evaluation"]
required_artifacts = ["evaluation"]
prompt = """
# Evaluate Phase

Evaluate progress against the acceptance criteria from the plan.

## IMPORTANT: Run Verification First

Run verification commands FIRST. Report their actual output.
Do NOT claim tests pass based on reasoning alone. Always execute:
- Tests (if applicable): run the actual test suite
- Type checking (if applicable): run the type checker
- Linting (if applicable): run the linter
- Manual inspection of outputs

## Then Assess Progress

Based on ACTUAL tool output, determine:
1. Which acceptance criteria are met?
2. Which are not yet met?
3. What new work is needed based on what was learned?

## Outputs

Write a CUMULATIVE `evaluation.md` that includes:
- Current verification results (tool output, not reasoning)
- Summary of ALL previous iterations (what was tried, what worked, what didn't)
- Unmet criteria with specific analysis of why they remain

## Decision

- If ALL acceptance criteria are met: transition to **complete**
- If criteria remain unmet: transition to **generate-tasks**
"""
suggested_next = [
    "complete",
    "generate-tasks"
]

[[phases]]
id = "generate-tasks"
context_artifacts = ["research", "implementation-plan", "evaluation"]
use_tasks = false
prompt = """
# Generate Tasks Phase

Based on the evaluation (injected above), generate the next batch of tasks.

## Read the Evaluation

Understand from the evaluation artifact:
- What acceptance criteria are not yet met
- What was learned during ALL previous iterations (evaluation includes cumulative history)
- What new work is needed
- What approaches have already been tried (do NOT repeat them)

## Check Dead-Ends

Dead-ends from failed approaches are injected above (if any exist).
Do NOT generate tasks that repeat failed approaches.

## Generate Tasks

Create tasks.json with tasks targeting the unmet criteria.
Each task should be scoped to fit in a single agent session (~10 steps max).

## Transition

After writing tasks.json, transition back to execute:

```bash
uv run plan.py loop-to-phase execute --reason "Iteration N: <summary of unmet criteria>"
```

This ensures proper re-entry context is maintained.
"""
suggested_next = ["execute"]

[[phases]]
id = "complete"
terminal = true
context_artifacts = ["evaluation"]
prompt = """
# Complete Phase

All acceptance criteria met. Summarize the work completed.

## Review CLAUDE.md Proposals

Proposals are auto-collected from execute phases. Check for pending proposals:

```bash
uv run ~/.claude-plugins/jons-plan/plan.py list-proposals
```

For each pending proposal, present to user via AskUserQuestion:
- Show: target file, proposed content, rationale
- Options: Accept, Reject
- If Accept: use Edit tool to apply the change to the target file
- Update status: `uv run ~/.claude-plugins/jons-plan/plan.py update-proposal-status <id> accepted|rejected`

## Review Challenges

```bash
uv run ~/.claude-plugins/jons-plan/plan.py list-challenges
```

For each pending challenge, present to user for acknowledgement.

## Final Summary

Summarize:
- The goal and how it was achieved
- Number of iterations taken
- Key learnings from the iteration journey
- Any follow-up work recommended

This is a terminal phase. The workflow is complete.
"""
