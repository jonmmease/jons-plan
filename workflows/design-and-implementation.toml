# Design-and-Implementation Workflow
# Design first, then optionally implement after user approval

[workflow]
name = "design-and-implementation"
description = "Design a solution, get user approval, then optionally implement"

[[phases]]
id = "research-planning"
prompt_files = ["research-planning"]
context_artifacts = ["research-brief", "research"]
required_artifacts = ["research-plan"]
prompt = """
## Tasks

1. Identify what needs to be researched for this request
2. List specific questions to answer
3. Identify sources (codebase, web, docs)
4. Include verification capability assessment as a research question

## Outputs

Write `research-plan.md` with:
- Questions to answer (grouped by domain)
- Sources to consult
- Research task assignments
- Verification question: "What test/lint/type-check/MCP tools are available?"
"""
suggested_next = [
    { phase = "research", instruction = "Research plan ready — begin research" }
]

[[phases]]
id = "research"
use_tasks = true
supports_prototypes = true
supports_cache_reference = true
context_artifacts = ["research-plan", "research"]
required_artifacts = ["research"]
prompt = """
# Research Phase

Execute the research plan (injected above).

If prior research findings are injected above (from a previous iteration), build on them — add new findings without removing existing content.

## Tasks
1. Explore the existing codebase
2. Research potential approaches
3. Identify constraints and requirements
4. Document trade-offs
5. **Assess verification capabilities**: What test frameworks, MCP servers, build/lint/type-check commands exist?

## Verification-Aware Research

**Prefer approaches that can be self-verified.** When comparing options:
- Favor technologies with existing test patterns in the codebase
- Prefer architectures that work with available MCP tools
- Consider: "How will I verify this actually works?"

## Outputs
Write `research.md` with findings, including a Verification Capabilities section.

Use prototype tasks if you need to answer questions through implementation.

If this is a subsequent iteration, append new findings to the existing research.md content.
"""
suggested_next = [
    { phase = "evaluate-research", instruction = "Research complete — evaluate findings" }
]

[[phases]]
id = "evaluate-research"
max_retries = 4
prompt_files = ["evaluate-research"]
context_artifacts = ["research-plan", "research"]
suggested_next = [
    { phase = "draft", instruction = "All questions adequately answered" },
    { phase = "research-planning", instruction = "Significant gaps remain — plan additional research for unanswered questions" }
]

[[phases]]
id = "draft"
use_tasks = true
planning_panel = true
context_artifacts = ["research"]
required_artifacts = ["design", "verification-plan"]
required_tasks = [
  { id = "opus-planning", description = "Generate design document via Opus", prompt_file = "dual-planning", model = "opus", inject_phase_prompt = true, context_artifacts = ["research"] },
  { id = "codex-planning", description = "Generate design document via Codex CLI", prompt_file = "dual-planning", executor = "codex-cli", inject_phase_prompt = true, inject_project_context = true, context_artifacts = ["research"] },
  { id = "gemini-planning", description = "Generate design document via Gemini CLI", prompt_file = "dual-planning", executor = "gemini-cli", inject_phase_prompt = true, inject_project_context = true, context_artifacts = ["research"] },
  { id = "synthesize-plans", description = "Judge and merge all designs", prompt_file = "plan-synthesis", parents = ["opus-planning", "codex-planning", "gemini-planning"], model = "opus" },
]
prompt = """
# Draft Phase

Create the design document based on the research findings (injected above).

## Planning Tasks
1. Synthesize research into a design
2. Document the proposed solution
3. Include implementation plan preview
4. **Create verification plan** (see below)

## Outputs

### design.md
Write the complete design document.

### verification-plan.md
Based on the verification capabilities identified in research, create a concrete plan:

```markdown
# Verification Plan

## Automated Checks
Commands to run after implementation:
- [ ] `<test command>` - <what it verifies>
- [ ] `<type check command>` - <what it verifies>
- [ ] `<lint command>` - <what it verifies>
- [ ] `<build command>` - <what it verifies>

## Interactive Verification
If MCP tools are available (browser, API, etc.):
- [ ] <step> - <expected result>
- [ ] <step> - <expected result>

## Acceptance Criteria
Specific criteria that must be true:
- [ ] <criterion derived from requirements>
- [ ] <criterion derived from design>
```

Output filename: design.md
"""
suggested_next = [
    { phase = "user-decision", instruction = "Design synthesized — present to user" }
]

[[phases]]
id = "revise"
context_artifacts = ["design"]
required_artifacts = ["design"]
prompt_files = ["revise"]
prompt = """
This is a work phase - do NOT present output to the user.

## Inputs
- design.md (injected above)
- User guidance (if re-entering from user-decision)

## Check for User Guidance

If the user provided feedback when selecting revise, retrieve it:
```bash
uv run ~/.claude-plugins/jons-plan/plan.py get-user-guidance
```

Address their specific concerns in the revision. After processing, clear it:
```bash
uv run ~/.claude-plugins/jons-plan/plan.py clear-user-guidance
```

## Tasks
1. Read user guidance if present
2. Update design.md with requested changes
3. Investigate uncertain items, update design based on findings

## Outputs
Updated design.md.
"""
suggested_next = [
    { phase = "user-decision", instruction = "Revisions complete — present to user" }
]

[[phases]]
id = "user-decision"
prompt = """
# User Decision Phase

Evaluate the design and decide whether to proceed with implementation.

## Design Evaluation

Review the design.md document and assess:

1. **Completeness**: Does the design answer all key questions from research?
2. **Clarity**: Is the approach well-defined and actionable?
3. **Feasibility**: Are the proposed changes realistic given the codebase?
4. **Verification**: Is there a concrete plan to validate the implementation?

## Decision Criteria

**Proceed to implementation if:**
- Design addresses the core requirements
- Approach is technically sound (no obvious blockers)
- Implementation steps are clear enough to begin work
- Verification plan exists and is executable

**Request revision if:**
- Critical gaps in the design (missing components, unclear decisions)
- Significant technical concerns not addressed
- Implementation steps too vague to execute

**Skip implementation if:**
- User explicitly requested design-only
- Implementation would require external dependencies not available

## Default Behavior

Unless there are significant concerns, **proceed to implementation**. Minor imperfections can be addressed during implementation through the proposal/challenge system.

## Output

State your decision and reasoning:
```markdown
## Design Evaluation

**Completeness**: [assessment]
**Clarity**: [assessment]
**Feasibility**: [assessment]
**Verification**: [assessment]

**Decision**: [Proceed to implement / Request revision / Complete design-only]
**Reasoning**: [1-2 sentences]
```

Then transition to the appropriate phase.
"""
user_review_artifacts = ["design.md"]
suggested_next = [
    { phase = "implement", instruction = "Design is solid — begin implementation" },
    { phase = "complete-design-only", instruction = "Design complete — skip implementation" },
    { phase = "revise", instruction = "Design needs changes" },
    { phase = "research", instruction = "Restart research preserving current learnings", requires_approval = true, approval_prompt = "Return to research phase? This will start fresh research while preserving learnings from the current design." }
]

[[phases]]
id = "implement"
use_tasks = true
max_retries = 3
context_artifacts = ["design"]
required_json_artifacts = ["proposals", "challenges"]
prompt_files = ["implement"]
prompt = """
## Task Schema - context_artifacts

When creating tasks, use `context_artifacts` to give tasks access to relevant documents:

```json
{
  "id": "implement-feature",
  "description": "Implement the feature",
  "context_artifacts": ["request", "design"],
  ...
}
```

**When to use context_artifacts:**
- Implementation tasks → `["request", "design"]` (need requirements + design)
- Validation tasks → `["request", "design"]` (to verify against both)

Common artifacts in this workflow:
- `request` - Original user request
- `research` - Research findings
- `design` - Approved design document
"""
suggested_next = [
    { phase = "validate", instruction = "All tasks complete — run verification" },
    { phase = "implement", instruction = "Task blocked — retry with adjusted approach" },
    { phase = "user-decision", instruction = "Reassess design based on implementation findings", requires_approval = true, approval_prompt = "Return to user-decision to reassess the design based on implementation findings?" }
]

[[phases]]
id = "validate"
prompt_files = ["validate"]
context_artifacts = ["verification-plan"]
suggested_next = [
    { phase = "complete", instruction = "All checks pass" },
    { phase = "implement", instruction = "Checks failed — fix and retry" },
    { phase = "user-decision", instruction = "Reassess design based on validation findings", requires_approval = true, approval_prompt = "Return to user-decision to reassess the design based on validation findings?" }
]

[[phases]]
id = "complete"
prompt_files = ["proposals-and-challenges", "complete-implementation"]
terminal = true

[[phases]]
id = "complete-design-only"
prompt = """
# Complete Design Only

Finalize without implementation.

The design document is complete. Implementation can be done
separately using the `/jons-plan:new` command with this design
as reference.

This is a terminal phase.
"""
terminal = true
