# Design-and-Implementation Workflow
# Design first, then optionally implement after user approval

[workflow]
name = "design-and-implementation"
description = "Design a solution, get user approval, then optionally implement"

[[phases]]
id = "research-planning"
prompt_files = ["research-planning"]
context_artifacts = ["research"]
required_artifacts = ["research-plan"]
prompt = """
## Tasks

1. Identify what needs to be researched for this request
2. List specific questions to answer
3. Identify sources (codebase, web, docs)
4. Include verification capability assessment as a research question

## Outputs

Write `research-plan.md` with:
- Questions to answer (grouped by domain)
- Sources to consult
- Research task assignments
- Verification question: "What test/lint/type-check/MCP tools are available?"
"""
suggested_next = [
    { phase = "research", instruction = "Research plan ready — begin research" }
]

[[phases]]
id = "research"
use_tasks = true
supports_prototypes = true
supports_cache_reference = true
context_artifacts = ["research-plan", "research"]
required_artifacts = ["research"]
prompt = """
# Research Phase

Execute the research plan (injected above).

If prior research findings are injected above (from a previous iteration), build on them — add new findings without removing existing content.

## Tasks
1. Explore the existing codebase
2. Research potential approaches
3. Identify constraints and requirements
4. Document trade-offs
5. **Assess verification capabilities**: What test frameworks, MCP servers, build/lint/type-check commands exist?

## Verification-Aware Research

**Prefer approaches that can be self-verified.** When comparing options:
- Favor technologies with existing test patterns in the codebase
- Prefer architectures that work with available MCP tools
- Consider: "How will I verify this actually works?"

## Outputs
Write `research.md` with findings, including a Verification Capabilities section.

Use prototype tasks if you need to answer questions through implementation.

If this is a subsequent iteration, append new findings to the existing research.md content.
"""
suggested_next = [
    { phase = "evaluate-research", instruction = "Research complete — evaluate findings" }
]

[[phases]]
id = "evaluate-research"
max_retries = 4
prompt_files = ["evaluate-research"]
context_artifacts = ["research-plan", "research"]
suggested_next = [
    { phase = "draft", instruction = "All questions adequately answered" },
    { phase = "research-planning", instruction = "Significant gaps remain — plan additional research for unanswered questions" }
]

[[phases]]
id = "draft"
context_artifacts = ["research"]
required_artifacts = ["design", "verification-plan"]
prompt = """
# Draft Phase

Create the design document based on the research findings (injected above).

## Tasks
1. Synthesize research into a design
2. Document the proposed solution
3. Include implementation plan preview
4. **Create verification plan** (see below)

## Outputs

### design.md
Write the complete design document.

### verification-plan.md
Based on the verification capabilities identified in research, create a concrete plan:

```markdown
# Verification Plan

## Automated Checks
Commands to run after implementation:
- [ ] `<test command>` - <what it verifies>
- [ ] `<type check command>` - <what it verifies>
- [ ] `<lint command>` - <what it verifies>
- [ ] `<build command>` - <what it verifies>

## Interactive Verification
If MCP tools are available (browser, API, etc.):
- [ ] <step> - <expected result>
- [ ] <step> - <expected result>

## Acceptance Criteria
Specific criteria that must be true:
- [ ] <criterion derived from requirements>
- [ ] <criterion derived from design>
```
"""
suggested_next = [
    { phase = "review", instruction = "Draft ready — send for external review" }
]

[[phases]]
id = "review"
use_tasks = true
prompt_files = ["selective-re-review"]
context_artifacts = ["design", "verification-plan", "review-feedback"]
required_artifacts = ["review-feedback"]
prompt = """
# Review Phase

Get external feedback on the design using reviewer agents.

## Setup

Create tasks.json with reviewer tasks. On first review, create tasks for all reviewers. On re-review, only create tasks for reviewers with unresolved Critical items.

### Example tasks.json

```json
[
  {
    "id": "gemini-review",
    "description": "Get design feedback from Gemini",
    "subagent": "gemini-reviewer",
    "subagent_prompt": "Review the design for architectural concerns, completeness, and clarity. Tag each item as Critical, Suggestion, or Nit.",
    "context_artifacts": ["design", "verification-plan"],
    "parents": [],
    "status": "todo"
  },
  {
    "id": "codex-review",
    "description": "Get design feedback from Codex",
    "subagent": "codex-reviewer",
    "subagent_prompt": "Review the design for technical accuracy and implementation feasibility. Tag each item as Critical, Suggestion, or Nit.",
    "context_artifacts": ["design", "verification-plan"],
    "parents": [],
    "status": "todo"
  },
  {
    "id": "synthesize-feedback",
    "description": "Synthesize feedback into review-feedback.md",
    "parents": ["gemini-review", "codex-review"],
    "steps": [
      "Read output from parent tasks",
      "Organize by source, then by severity: Critical, Suggestion, Nit",
      "Write review-feedback.md"
    ],
    "status": "todo"
  }
]
```

## Task Execution

1. Run reviewer tasks in parallel (they have no parents)
2. After both complete, run synthesize task
3. Mark each task done as it completes

## Outputs

Write `review-feedback.md` with severity-tagged feedback from all reviewers (Critical/Suggestion/Nit).
"""
suggested_next = [
    { phase = "revise", instruction = "Feedback received — incorporate changes" }
]

[[phases]]
id = "revise"
context_artifacts = ["design", "review-feedback"]
required_artifacts = ["design"]
prompt_files = ["revise"]
prompt = """
This is a work phase - do NOT present output to the user.

## Inputs
- design.md (injected above)
- review-feedback.md (injected above)
- User guidance (if re-entering from user-decision)

## Check for User Guidance

If the user provided feedback when selecting revise, retrieve it:
```bash
uv run ~/.claude-plugins/jons-plan/plan.py get-user-guidance
```

Address their specific concerns in the revision. After processing, clear it:
```bash
uv run ~/.claude-plugins/jons-plan/plan.py clear-user-guidance
```

## Tasks
1. Read user guidance if present
2. Update design.md with accepted changes
3. Investigate uncertain items, update design based on findings
4. Update confidence assessment

## Outputs
Updated design.md.
"""
suggested_next = [
    { phase = "evaluate-revisions", instruction = "Revisions complete — evaluate changes" }
]

[[phases]]
id = "evaluate-revisions"
max_retries = 2
prompt_files = ["evaluate-revisions"]
context_artifacts = ["design", "review-feedback"]
suggested_next = [
    { phase = "user-decision", instruction = "All critical items resolved" },
    { phase = "review", instruction = "Critical items remain unresolved — re-engage only reviewers with unresolved Critical items" }
]

[[phases]]
id = "user-decision"
prompt = """
# User Decision Phase

Present the design to the user for approval.

## User Review
The user should review:
- design.md - The proposed solution
- review-feedback.md - External feedback received
"""
requires_user_input = true
user_review_artifacts = ["design.md", "review-feedback.md"]
suggested_next = [
    { phase = "implement", instruction = "Design approved — begin implementation" },
    { phase = "complete-design-only", instruction = "Design approved — skip implementation" },
    { phase = "revise", instruction = "Design needs changes" },
    { phase = "research", instruction = "Restart research preserving current learnings", requires_approval = true, approval_prompt = "Return to research phase? This will start fresh research while preserving learnings from the current design." }
]

[[phases]]
id = "implement"
use_tasks = true
on_blocked = "self"
max_retries = 3
context_artifacts = ["design"]
required_json_artifacts = ["proposals", "challenges"]
prompt_files = ["implement"]
prompt = """
## Task Schema - context_artifacts

When creating tasks, use `context_artifacts` to give tasks access to relevant documents:

```json
{
  "id": "implement-feature",
  "description": "Implement the feature",
  "context_artifacts": ["request", "design"],
  ...
}
```

**When to use context_artifacts:**
- Implementation tasks → `["request", "design"]` (need requirements + design)
- Validation tasks → `["request", "design"]` (to verify against both)

Common artifacts in this workflow:
- `request` - Original user request
- `research` - Research findings
- `design` - Approved design document
"""
suggested_next = [
    { phase = "validate", instruction = "All tasks complete — run verification" },
    { phase = "user-decision", instruction = "Reassess design based on implementation findings", requires_approval = true, approval_prompt = "Return to user-decision to reassess the design based on implementation findings?" }
]

[[phases]]
id = "validate"
prompt_files = ["validate"]
context_artifacts = ["verification-plan"]
suggested_next = [
    { phase = "complete", instruction = "All checks pass" },
    { phase = "user-decision", instruction = "Reassess design based on validation findings", requires_approval = true, approval_prompt = "Return to user-decision to reassess the design based on validation findings?" }
]
on_blocked = "implement"

[[phases]]
id = "complete"
prompt_files = ["proposals-and-challenges", "complete-implementation"]
terminal = true

[[phases]]
id = "complete-design-only"
prompt = """
# Complete Design Only

Finalize without implementation.

The design document is complete. Implementation can be done
separately using the `/jons-plan:new` command with this design
as reference.

This is a terminal phase.
"""
terminal = true
