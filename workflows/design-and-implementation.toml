# Design-and-Implementation Workflow
# Design first, then optionally implement after user approval

[workflow]
name = "design-and-implementation"
description = "Design a solution, get user approval, then optionally implement"

[[phases]]
id = "research-planning"
context_artifacts = ["research"]
required_artifacts = ["research-plan"]
prompt = """
# Research Planning Phase

Plan the research before diving in.

## Prior Research

If prior research findings are injected above (from a previous iteration), review them to identify what's already been answered. Focus the new research plan on unanswered questions only.

## Tasks

1. Identify what needs to be researched for this request
2. List specific questions to answer
3. Identify sources (codebase, web, docs)
4. Include verification capability assessment as a research question

## Outputs

Write `research-plan.md` with:
- Questions to answer (grouped by domain)
- Sources to consult
- Research task assignments
- Verification question: "What test/lint/type-check/MCP tools are available?"
"""
suggested_next = ["research"]

[[phases]]
id = "research"
use_tasks = true
supports_prototypes = true
supports_cache_reference = true
context_artifacts = ["research-plan", "research"]
required_artifacts = ["research"]
prompt = """
# Research Phase

Execute the research plan (injected above).

If prior research findings are injected above (from a previous iteration), build on them — add new findings without removing existing content.

## Tasks
1. Explore the existing codebase
2. Research potential approaches
3. Identify constraints and requirements
4. Document trade-offs
5. **Assess verification capabilities**: What test frameworks, MCP servers, build/lint/type-check commands exist?

## Verification-Aware Research

**Prefer approaches that can be self-verified.** When comparing options:
- Favor technologies with existing test patterns in the codebase
- Prefer architectures that work with available MCP tools
- Consider: "How will I verify this actually works?"

## Outputs
Write `research.md` with findings, including a Verification Capabilities section.

Use prototype tasks if you need to answer questions through implementation.

If this is a subsequent iteration, append new findings to the existing research.md content.
"""
suggested_next = ["evaluate-research"]

[[phases]]
id = "evaluate-research"
max_retries = 4
context_artifacts = ["research-plan", "research"]
prompt = """
# Evaluate Research Phase

Evaluate whether the research adequately answers the questions from the research plan.

## Process

1. Read the research plan (injected above) and list each question
2. Read the research findings (injected above)
3. For each question, assess: answered, partially answered, or unanswered

## Decision

- If all questions are adequately answered: transition to **draft**
- If significant gaps remain: transition to **research-planning** to plan additional research
"""
suggested_next = ["draft", "research-planning"]

[[phases]]
id = "draft"
context_artifacts = ["research"]
required_artifacts = ["design", "verification-plan"]
prompt = """
# Draft Phase

Create the design document based on the research findings (injected above).

## Tasks
1. Synthesize research into a design
2. Document the proposed solution
3. Include implementation plan preview
4. **Create verification plan** (see below)

## Outputs

### design.md
Write the complete design document.

### verification-plan.md
Based on the verification capabilities identified in research, create a concrete plan:

```markdown
# Verification Plan

## Automated Checks
Commands to run after implementation:
- [ ] `<test command>` - <what it verifies>
- [ ] `<type check command>` - <what it verifies>
- [ ] `<lint command>` - <what it verifies>
- [ ] `<build command>` - <what it verifies>

## Interactive Verification
If MCP tools are available (browser, API, etc.):
- [ ] <step> - <expected result>
- [ ] <step> - <expected result>

## Acceptance Criteria
Specific criteria that must be true:
- [ ] <criterion derived from requirements>
- [ ] <criterion derived from design>
```
"""
suggested_next = ["review"]

[[phases]]
id = "review"
use_tasks = true
prompt = """
# Review Phase

Get external feedback on the design using reviewer agents.

## Setup

Create tasks.json with reviewer tasks. Reviewer tasks can run in parallel since they have no dependencies on each other.

### Example tasks.json

```json
[
  {
    "id": "gemini-review",
    "description": "Get design feedback from Gemini",
    "subagent": "gemini-reviewer",
    "subagent_prompt": "Review the design for architectural concerns, completeness, and clarity",
    "context_artifacts": ["design", "verification-plan"],
    "parents": [],
    "status": "todo"
  },
  {
    "id": "codex-review",
    "description": "Get design feedback from Codex",
    "subagent": "codex-reviewer",
    "subagent_prompt": "Review the design for technical accuracy and implementation feasibility",
    "context_artifacts": ["design", "verification-plan"],
    "parents": [],
    "status": "todo"
  },
  {
    "id": "synthesize-feedback",
    "description": "Synthesize feedback into review-feedback.md",
    "parents": ["gemini-review", "codex-review"],
    "steps": [
      "Read output from parent tasks",
      "Categorize feedback: critical, suggestion, nit",
      "Write review-feedback.md"
    ],
    "status": "todo"
  }
]
```

## Task Execution

1. Run reviewer tasks in parallel (they have no parents)
2. After both complete, run synthesize task
3. Mark each task done as it completes

## Outputs

Write `review-feedback.md` with categorized feedback from all reviewers.
"""
suggested_next = ["revise"]

[[phases]]
id = "revise"
prompt = """
# Revise Phase

Incorporate reviewer feedback into design.md. This is a work phase - do NOT present output to the user. The user will review in the next phase.

## Inputs
- design.md from previous draft/revise
- review-feedback.md from external reviewers
- User guidance (if re-entering from user-decision)

## Check for User Guidance

If the user provided feedback when selecting revise, retrieve it:
```bash
uv run ~/.claude-plugins/jons-plan/plan.py get-user-guidance
```

Address their specific concerns in the revision. After processing, clear it:
```bash
uv run ~/.claude-plugins/jons-plan/plan.py clear-user-guidance
```

## Tasks
1. Read user guidance if present
2. Address feedback in design.md (focus on user's concerns)
3. Update confidence assessment

## Outputs
Updated design.md.

## Transition
After updating design.md, immediately transition to user-decision phase. Do NOT summarize or present the changes - the user will review in the next phase.
"""
suggested_next = ["user-decision"]

[[phases]]
id = "user-decision"
prompt = """
# User Decision Phase

Present the design to the user for approval.

## User Review
The user should review:
- design.md - The proposed solution
- review-feedback.md - External feedback received
"""
requires_user_input = true
user_review_artifacts = ["design.md", "review-feedback.md"]
suggested_next = [
    "implement",
    "complete-design-only",
    "revise",
    { phase = "research", requires_approval = true, approval_prompt = "Return to research phase? This will start fresh research while preserving learnings from the current design." }
]

[[phases]]
id = "implement"
use_tasks = true
on_blocked = "self"
max_retries = 3
context_artifacts = ["design"]
required_json_artifacts = ["proposals", "challenges"]
prompt = """
# Implement Phase

Execute the design (injected above).

## Setup
1. Create tasks.json with implementation plan based on the design
2. All tasks start with `status: "todo"`

## Task Schema - context_artifacts

When creating tasks, use `context_artifacts` to give tasks access to relevant documents:

```json
{
  "id": "implement-feature",
  "description": "Implement the feature",
  "context_artifacts": ["request", "design"],
  ...
}
```

**When to use context_artifacts:**
- Implementation tasks → `["request", "design"]` (need requirements + design)
- Validation tasks → `["request", "design"]` (to verify against both)

Common artifacts in this workflow:
- `request` - Original user request
- `research` - Research findings
- `design` - Approved design document

## Task Execution
1. Execute tasks following the design
2. Set each task to in-progress before starting
3. Mark tasks done immediately after completion

## Outputs
Working implementation of the design.
"""
suggested_next = [
    "validate",
    { phase = "user-decision", requires_approval = true, approval_prompt = "Return to user-decision to reassess the design based on implementation findings?" }
]

[[phases]]
id = "validate"
context_artifacts = ["verification-plan"]
prompt = """
# Validate Phase

Verify implementation matches the design using the verification plan (injected above).

## Execution

### 1. Run Automated Checks
Execute each command from the verification plan:
- Run test suite
- Run type checker
- Run linter
- Run build

Log results for each check.

### 2. Interactive Verification (if applicable)
If the verification plan includes MCP-based checks:
- Execute browser automation steps
- Verify UI behavior matches design

### 3. Acceptance Criteria
Verify each criterion from the verification plan (derived from design.md) is met.

## Outcomes

**All checks pass:** Transition to complete.

**Checks fail:**
- If fixable: Fix and re-run checks
- If needs rework: Transition back to implement
"""
suggested_next = [
    "complete",
    { phase = "user-decision", requires_approval = true, approval_prompt = "Return to user-decision to reassess the design based on validation findings?" }
]
on_blocked = "implement"

[[phases]]
id = "complete"
prompt = """
# Complete Phase

Finalize the implementation.

## Review CLAUDE.md Proposals

Proposals are auto-collected from implement phases. Check for pending proposals:

```bash
uv run ~/.claude-plugins/jons-plan/plan.py list-proposals
```

For each pending proposal, present to user via AskUserQuestion:
- Show: target file, proposed content, rationale
- Options: Accept, Reject
- If Accept: use Edit tool to apply the change to the target file
- Update status: `uv run ~/.claude-plugins/jons-plan/plan.py update-proposal-status <id> accepted|rejected`

## Review Challenges

Challenges are auto-collected from implement phases. Check for pending challenges:

```bash
uv run ~/.claude-plugins/jons-plan/plan.py list-challenges
```

For each pending challenge, present to user for acknowledgement:
- Show: title, what was attempted, what went wrong, workaround used
- No action needed - just acknowledge
- Update status: `uv run ~/.claude-plugins/jons-plan/plan.py acknowledge-challenge <id>`

## Final Tasks
1. Review all changes
2. Prepare commit message
3. Document any follow-up work

This is a terminal phase.
"""
terminal = true

[[phases]]
id = "complete-design-only"
prompt = """
# Complete Design Only

Finalize without implementation.

The design document is complete. Implementation can be done
separately using the `/jons-plan:new` command with this design
as reference.

This is a terminal phase.
"""
terminal = true
